\section{Examples}
Now we are set up on HPC and have installed some of the packaged we need, we will go through some examples on how to get the most out of HPC.
%
%
%
\subsection{Multicore Processing - Let the OS do the Hard Work}
%
%
Many times when we want to process a large data set, we want to do a single task to each element in the data set, and sometimes this individual operation can be computationally expensive. An example is preprocessing all images in a large data set to remove certain artefacts, convert to a more convenient format etc.. It would be beneficial to process many of these items in the many available CPU cores on HPC. One method is to write a multi-threaded/multi-process script (not a simple task in R) to process the data. Another and far easier way to handle this is to create a script that processes a single item in the data set, and submit this job many times to the HPC cluster with a different observation from the data set as an input example. The idea is to let the OS and the scheduler do the hard part of organising multicore processing.
%
%
%
\par
%
%
Another scenario when this type of processing is helpful is for model validation. Consider a case where you are commencing work on a new project, with a new type of data set and you want to run some experiments to bench mark the performance of different models with different parameters. For example, say you are fitting a mixture model, and you want to investigate the performance of the model for different number of mixtures, or a boosted regression tree, where you want to see how the accuracy of your predictions change when altering the parameters of the model. This type of experimentation can greatly benefit from this type of parallel processing, where you want to run several independent experiments with varying parameters. An example of how to do this is supplied in the \hltexttt{bt\_examples} directory of the repo for this guide, where we will fit many different boosted regression tree models to try and predict the presence of breast cancer based on biopsy information \cite{breast}.
%
%
%
\subsubsection{Example: Experimenting with Different Model Configurations in Parallel}
%
%
When looking at the contents of the \hltexttt{bt\_examples} directory, you will find a single R script \hltexttt{breast\_cancer\_bt.R} and ten batch scripts \hltexttt{batch\_bt\_XXXX.sh}. Each of these batch scripts will invoke the \hltexttt{breast\_cancer\_bt.R} script, though each script will supply a different command line argument to specify the number of trees we want to use in the model. We can sumbit batch jobs for all of these scripts using the following commands,
\\
\par
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  #Change to the bt_examples directory
  cd ~/hpc_guide/bt_examples/
  #now lets send all the batch scripts to qsub
  #so we can submit jobs for them
  #
  #We can use a for loop to submit all scripts
  #that end in .sh
  for sub in $( ls ./*.sh); do qsub $sub; done
\end{minted}
% 
\\
% 
After running this command, you will see that ten independent jobs have been submitted. You can track the progress of these jobs by again running the command,
\\
\par
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  watch -n 1 qstat -u <your_QUT_username>
\end{minted}
%
\\
% 
\begin{story}
  \textbf{NOTE:}
  \\
  This specific model validation is here only as an example. If you look at the \hltexttt{stdout.out} files for each different model using the instructions below, you will see that each model performs exactly the same. You can have a play with this with different types of models, such as mixture models. You could try fitting a number of different models, each with a different number of mixtures and see which model performs best.
\end{story}

%
%
Once all of the jobs are completed, you will see a number of log files have been created, a standard output and a standard error file for each job submitted. You can view the output of these files using the cat command.
%
%
\\
\par
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  #view the output of a single job
  cat bt_10000_stdout.out
  cat bt_10000_stderr.out
  #if the output file is long, you can display it
  #in a slightly nicer format where you can scroll through
  #using enter or the space bar
  cat bt_10000_stdout.out | more
\end{minted}
%
\\
% 
% 
After running these jobs, you may want to remove the current stash of output files. You can do this using the \hltexttt{rm} command, though this should be used with caution. This command will remove files for good, and unlike your desktop system, after you remove a file in a UNIX like system, it is gone for good! This is another reason why you should use version control systems such as Git with remote back ups. If you accidentally delete all of your source files and you haven't backed them up using Git or something similar, they will be gone forever and you will have to start again from scratch!.
%
%
\par
%
%
I stress the importance/danger associated with using the \hltexttt{rm} command to hopefully help you avoid disaster. In saying that, if used properly it is a simple and extremely useful command.
%
%
\par
If you have named output files in the standard I have used throughout my examples (output scripts ending with \hltexttt{stdout.out} and \hltexttt{stderr.out}), then you can delete all of these files with the following commands,
%
\\
\par
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  #delete any files that end with stdout.out
  rm ./*stdout.out
  #delete any files that end with stderr.out
  rm ./*stderr.out
\end{minted}
%
%
\subsubsection{Example: Preprocessing Large Number of Files In Parallel}
%
%
Preprocessing functions are prime targets for parallelisation in this way, as we are usually assuming independence between samples in a lot of our models. I work a lot with images, and certain image preprocessing steps can be pretty time consuming, but this type of scheme could apply to any large data set where preprocessing is expensive (also images make it nicer to view \smiley)
%
%
\par
%
%
An example of expensive preprocessing methods in computer vision is computing depth (or disparity) maps from stereo images. The basic idea is that with a two cameras next to each other capturing the same scene, we can analyse them geometrically to compute depth similar to how we do. I have shown an example of what it looks like in Figure \ref{fig:disp}.
%
%
\begin{figure}[!h]
  \centering
  \subfloat[][]{\includegraphics[width=0.5\linewidth]{./figs/left.jpg}}
  \subfloat[][]{\includegraphics[width=0.5\linewidth]{./figs/right.jpg}}
  \\
  \subfloat[][]{\includegraphics[width=0.5\linewidth]{./figs/disp.jpg}}
  \caption{Examples of left and right images used to estimate depth from a scene.}
  \label{fig:disp}
\end{figure}
%
%
\par
%
%
For larger images, this can be reasonably expensive to compute, and if we have a large dataset it can take a really long time. These properties make it a prime candidate for the type of parallelisation where we make the OS do all the hard scheduling work for us.
%
%
\par
%
%
The basic idea with this type of processing is first a normal program is created which is fed a list of files to perform the preprocessing operations on. This program then iterates over the samples within the list one by one until it reaches the end. The way we can parallelise this, is instead of having one large list with all the files we need to process, we can split that file up in to multiple seperate files. That way, we can submit individual jobs that operate over their own subset of images to process, and can save a lot of time\footnote{I have done exactly this for a project, and it literally saved me days in compute time, at the expense of about 15 minutes extra work.}.
%
%
\par
%
%
Just to summarise, the basic steps we need to do are,
\begin{enumerate}
  \item Create a single list of all files/images to process
  \item Split that into multiple smaller files
  \item Submit a batch job which operates over the images in each of the smaller files
  \item Dance \smiley
\end{enumerate}
% 
% 
%
%As with most things, the best way to learn it is to put it into practice, so thats what we are going to do. An example in this guide is created under the \hltexttt{stereo\_images\_example} that does this to a subset of real images from the Holopix Dataset [Citation]. Before looking at the code, we will copy the data across to where we can access it. It is currently on HPC in a shared directory, but is handy to copy across locally for us.
%
% %
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  # copy the data to the local directory
  mkdir data
  cp /work/SAIVT/hpc_guide_data/stereo_images/holopix50k/data/small/* ./data
  # data will now be stored in the following structure
  #  | image_key
  #  |-- left.jpg
  #  |-- right.jpg
  # so within data there will be sub directories, and each sub directory will
  # have a left/right image pair
\end{minted}

%
% Now we want to create a single file that contains the directories of all those left/right image pairs. Is also handy to list it as an absolute path, as it makes it easier to work with. We then want to split all of the samples listed in that file into multiple smaller files. To do this, we can use the \hltexttt{split} command.
%
%
\begin{minted}[bgcolor=Light, frame=single, fontsize=\footnotesize, fontfamily=courier]{bash}
  # create one big list of all samples
  ls -d $PWD/data > all_image_pairs.txt
  # now split into multiple smaller files
  # Here I am saying make it so that there are two lines per file
  # This is a very small amount in reality but this is just to illustrate
  # how to do it. 
  split -l 2 -d image_pair_paths.txt image_ --additional-suffix=.txt
\end{minted}
%
%
%
%If we have a look now, there will be 10 new files named \hltexttt{image\_00.txt, image\_01.txt ...}. These are the smaller lists of files to operate over, so we can submit a single job for each of these files.

%\subsubsubsection{}

\subsubsection{Tips for bulk Submitting Jobs}
Bulk submitting jobs in this way relies on you designing your original code to handle command line arguments. Adopting this type of program design is extremely helpful during experimentation, and is a part of general good coding practices. Don't hard-code anything you think may even have the slightest possibility of ever changing. Command line arguments are a great way to develop software that is highly modular, and generally easy to use (as long as you document what you have done!).
%
%
\par
%
%
For bulk submitting jobs in this way, I would recommend using the bash scripts I have provided as a template, and simply modifying them to suit your needs. The components that you will need to modify include the PBS directives at the top that define your computational requirements. Another important component to change is the output location where the standard output and standard error files will be saved. If every script uses the same name for these output files, they will simply be overwritten whenever a new job is executed.
%
%
\par
Depending on the number of jobs you are planning to submit, it can also be helpful to write a small program that actually generated the qsub bash scripts for you. An easy way to do this is to start with a base file that has almost all the information you need, excluding the names of the output text files and the different input arguments you want to supply. Then you can write a small script that simply fills these areas with the information you require. If you want some examples on how I do this, just let me know and I can send you some examples.
%
%
\par
HPC will let you run roughly 100 different jobs simultaneously, though you are able to submit many many more jobs than that. A max of 10000 job submissions would be a reasonable limit, depending on the resources you require. If you do submit more than 100 jobs, excess jobs will simply join the cue and commence running after some of your other jobs have finished.
%
%
%
\subsection{Multicore Processing - The Hard Way}
Whilst the previous section described how to efficiently and easily parallelise your work, the bulk submission method is only suitable when individual tasks can be run independently. For many cases, this type of parallelisation is not possible. For these scenarios, parallelisation may still be feasible, as a few packages support multicore processing. In general, this a much more involved and arduous task, and one that I don't believe R handles nicely when compared to other programming languages such as Python\footnote{Although it can also be a pain to implement multithreaded programs in Python!}. Given the increased complexity associated with implementing many multiprocessing programs directly whithin R, it will not be covered in this guide, as it is assummed that if you have the programming proficiency to implement such a system, you should have little dramas migrating it to the HPC environment.


%%%Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
